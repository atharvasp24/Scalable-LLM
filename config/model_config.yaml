# Model configuration for LLaMA-2 and Mistral fine-tuning

model:
  base_model: "meta-llama/Llama-2-7b-hf"
  alt_model: "mistralai/Mistral-7B-v0.1"
  max_seq_length: 8192
  context_strategy: "chunked_attention"    # Options: chunked_attention, rope_interpolation
  use_gradient_checkpointing: true
  tokenizer: "meta-llama/Llama-2-7b-hf"

training:
  epochs: 3
  batch_size: 2
  learning_rate: 2e-5
  weight_decay: 0.01
  warmup_steps: 200
  logging_steps: 50
  save_steps: 500
  eval_steps: 200
  mixed_precision: bf16

data:
  train_file: "data/train.json"
  val_file: "data/val.json"
  retrieval_index: "data/faiss_index.bin"
